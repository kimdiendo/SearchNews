{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tải thư viện cần thiết"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from pyvi import ViPosTagger , ViTokenizer\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.tokenize import word_tokenize , sent_tokenize\n",
    "from wordcloud import STOPWORDS\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tiền xử lý dữ liệu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from pyvi import ViPosTagger , ViTokenizer\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize , sent_tokenize\n",
    "class BaseVNPreprocessor:\n",
    "    def __init__(self):\n",
    "        self.emoji_pattern = re.compile(\"[\"\n",
    "                            u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                            u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                            u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                            u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                            u\"\\U00002500-\\U00002BEF\"  # chinese char\n",
    "                            u\"\\U00002702-\\U000027B0\"\n",
    "                            u\"\\U00002702-\\U000027B0\"\n",
    "                            u\"\\U000024C2-\\U0001F251\"\n",
    "                            u\"\\U0001f926-\\U0001f937\"\n",
    "                            u\"\\U00010000-\\U0010ffff\"\n",
    "                            u\"\\u2640-\\u2642\"\n",
    "                            u\"\\u2600-\\u2B55\"\n",
    "                            u\"\\u200d\"\n",
    "                            u\"\\u23cf\"\n",
    "                            u\"\\u23e9\"\n",
    "                            u\"\\u231a\"\n",
    "                            u\"\\ufe0f\"  # dingbats\n",
    "                            u\"\\u3030\"\n",
    "                            \"]+\", flags=re.UNICODE)\n",
    "        with open('vietnamese-stopword_dashed.txt' , encoding='utf-8') as f:\n",
    "                 self.stopwords = f.readlines()\n",
    "        self.vowel_table = [['a', 'à', 'á', 'ả', 'ã', 'ạ', 'a'],\n",
    "                    ['ă', 'ằ', 'ắ', 'ẳ', 'ẵ', 'ặ', 'aw'],\n",
    "                    ['â', 'ầ', 'ấ', 'ẩ', 'ẫ', 'ậ', 'aa'],\n",
    "                    ['e', 'è', 'é', 'ẻ', 'ẽ', 'ẹ', 'e'],\n",
    "                    ['ê', 'ề', 'ế', 'ể', 'ễ', 'ệ', 'ee'],\n",
    "                    ['i', 'ì', 'í', 'ỉ', 'ĩ', 'ị', 'i'],\n",
    "                    ['o', 'ò', 'ó', 'ỏ', 'õ', 'ọ', 'o'],\n",
    "                    ['ô', 'ồ', 'ố', 'ổ', 'ỗ', 'ộ', 'oo'],\n",
    "                    ['ơ', 'ờ', 'ớ', 'ở', 'ỡ', 'ợ', 'ow'],\n",
    "                    ['u', 'ù', 'ú', 'ủ', 'ũ', 'ụ', 'u'],\n",
    "                    ['ư', 'ừ', 'ứ', 'ử', 'ữ', 'ự', 'uw'],\n",
    "                    ['y', 'ỳ', 'ý', 'ỷ', 'ỹ', 'ỵ', 'y']]\n",
    "        self.vowel_to_ids = {}\n",
    "        for i in range(len(self.vowel_table)):\n",
    "            for j in range(len(self.vowel_table[i]) - 1):\n",
    "                self.vowel_to_ids[self.vowel_table[i][j]] = (i, j)\n",
    "        self.uniChars = \"àáảãạâầấẩẫậăằắẳẵặèéẻẽẹêềếểễệđìíỉĩịòóỏõọôồốổỗộơờớởỡợùúủũụưừứửữựỳýỷỹỵÀÁẢÃẠÂẦẤẨẪẬĂẰẮẲẴẶÈÉẺẼẸÊỀẾỂỄỆĐÌÍỈĨỊÒÓỎÕỌÔỒỐỔỖỘƠỜỚỞỠỢÙÚỦŨỤƯỪỨỬỮỰỲÝỶỸỴÂĂĐÔƠƯ\"\n",
    "        self.unsignChars = \"aaaaaaaaaaaaaaaaaeeeeeeeeeeediiiiiooooooooooooooooouuuuuuuuuuuyyyyyAAAAAAAAAAAAAAAAAEEEEEEEEEEEDIIIOOOOOOOOOOOOOOOOOOOUUUUUUUUUUUYYYYYAADOOU\"\n",
    "\n",
    "    def loaddicchar(self):\n",
    "        dic = {}\n",
    "        char1252 = 'à|á|ả|ã|ạ|ầ|ấ|ẩ|ẫ|ậ|ằ|ắ|ẳ|ẵ|ặ|è|é|ẻ|ẽ|ẹ|ề|ế|ể|ễ|ệ|ì|í|ỉ|ĩ|ị|ò|ó|ỏ|õ|ọ|ồ|ố|ổ|ỗ|ộ|ờ|ớ|ở|ỡ|ợ|ù|ú|ủ|ũ|ụ|ừ|ứ|ử|ữ|ự|ỳ|ý|ỷ|ỹ|ỵ|À|Á|Ả|Ã|Ạ|Ầ|Ấ|Ẩ|Ẫ|Ậ|Ằ|Ắ|Ẳ|Ẵ|Ặ|È|É|Ẻ|Ẽ|Ẹ|Ề|Ế|Ể|Ễ|Ệ|Ì|Í|Ỉ|Ĩ|Ị|Ò|Ó|Ỏ|Õ|Ọ|Ồ|Ố|Ổ|Ỗ|Ộ|Ờ|Ớ|Ở|Ỡ|Ợ|Ù|Ú|Ủ|Ũ|Ụ|Ừ|Ứ|Ử|Ữ|Ự|Ỳ|Ý|Ỷ|Ỹ|Ỵ'.split(\n",
    "            '|')\n",
    "        charutf8 = \"à|á|ả|ã|ạ|ầ|ấ|ẩ|ẫ|ậ|ằ|ắ|ẳ|ẵ|ặ|è|é|ẻ|ẽ|ẹ|ề|ế|ể|ễ|ệ|ì|í|ỉ|ĩ|ị|ò|ó|ỏ|õ|ọ|ồ|ố|ổ|ỗ|ộ|ờ|ớ|ở|ỡ|ợ|ù|ú|ủ|ũ|ụ|ừ|ứ|ử|ữ|ự|ỳ|ý|ỷ|ỹ|ỵ|À|Á|Ả|Ã|Ạ|Ầ|Ấ|Ẩ|Ẫ|Ậ|Ằ|Ắ|Ẳ|Ẵ|Ặ|È|É|Ẻ|Ẽ|Ẹ|Ề|Ế|Ể|Ễ|Ệ|Ì|Í|Ỉ|Ĩ|Ị|Ò|Ó|Ỏ|Õ|Ọ|Ồ|Ố|Ổ|Ỗ|Ộ|Ờ|Ớ|Ở|Ỡ|Ợ|Ù|Ú|Ủ|Ũ|Ụ|Ừ|Ứ|Ử|Ữ|Ự|Ỳ|Ý|Ỷ|Ỹ|Ỵ\".split(\n",
    "            '|')\n",
    "        for i in range(len(char1252)):\n",
    "            dic[char1252[i]] = charutf8[i]\n",
    "        return dic\n",
    "    # Đưa toàn bộ dữ liệu qua hàm này để chuẩn hóa lại\n",
    "    def convert_unicode(self ,txt):\n",
    "        dicchar = self.loaddicchar()\n",
    "        return re.sub(\n",
    "            r'à|á|ả|ã|ạ|ầ|ấ|ẩ|ẫ|ậ|ằ|ắ|ẳ|ẵ|ặ|è|é|ẻ|ẽ|ẹ|ề|ế|ể|ễ|ệ|ì|í|ỉ|ĩ|ị|ò|ó|ỏ|õ|ọ|ồ|ố|ổ|ỗ|ộ|ờ|ớ|ở|ỡ|ợ|ù|ú|ủ|ũ|ụ|ừ|ứ|ử|ữ|ự|ỳ|ý|ỷ|ỹ|ỵ|À|Á|Ả|Ã|Ạ|Ầ|Ấ|Ẩ|Ẫ|Ậ|Ằ|Ắ|Ẳ|Ẵ|Ặ|È|É|Ẻ|Ẽ|Ẹ|Ề|Ế|Ể|Ễ|Ệ|Ì|Í|Ỉ|Ĩ|Ị|Ò|Ó|Ỏ|Õ|Ọ|Ồ|Ố|Ổ|Ỗ|Ộ|Ờ|Ớ|Ở|Ỡ|Ợ|Ù|Ú|Ủ|Ũ|Ụ|Ừ|Ứ|Ử|Ữ|Ự|Ỳ|Ý|Ỷ|Ỹ|Ỵ',\n",
    "            lambda x: dicchar[x.group()], txt)\n",
    "    #xử lý sign or old\n",
    "    def normalize_sign_of_a_word(self , word):\n",
    "        if not self.is_valid_vietnam_word(word):\n",
    "            return word\n",
    "        chars = list(word)\n",
    "        dau_cau = 0\n",
    "        vowel_index = []\n",
    "        qu_or_gi = False\n",
    "        for index, char in enumerate(chars):\n",
    "            x, y = self.vowel_to_ids.get(char, (-1, -1))\n",
    "            if x == -1:\n",
    "                continue\n",
    "            elif x == 9:  # check qu\n",
    "                if index != 0 and chars[index - 1] == 'q':\n",
    "                    chars[index] = 'u'\n",
    "                    qu_or_gi = True\n",
    "            elif x == 5:  # check gi\n",
    "                if index != 0 and chars[index - 1] == 'g':\n",
    "                    chars[index] = 'i'\n",
    "                    qu_or_gi = True\n",
    "            if y != 0:\n",
    "                dau_cau = y\n",
    "                chars[index] = self.vowel_table[x][0]\n",
    "            if not qu_or_gi or index != 1:\n",
    "                vowel_index.append(index)\n",
    "        if len(vowel_index) < 2:\n",
    "            if qu_or_gi:\n",
    "                if len(chars) == 2:\n",
    "                    x, y = self.vowel_to_ids.get(chars[1])\n",
    "                    chars[1] = self.vowel_table[x][dau_cau]\n",
    "                else:\n",
    "                    x, y = self.vowel_to_ids.get(chars[2], (-1, -1))\n",
    "                    if x != -1:\n",
    "                        chars[2] = self.vowel_table[x][dau_cau]\n",
    "                    else:\n",
    "                        chars[1] = self.vowel_table[5][dau_cau] if chars[1] == 'i' else self.vowel_table[9][dau_cau]\n",
    "                return ''.join(chars)\n",
    "            return word\n",
    "    \n",
    "        for index in vowel_index:\n",
    "            x, y = self.vowel_to_ids[chars[index]]\n",
    "            if x == 4 or x == 8:  # ê, ơ\n",
    "                chars[index] = self.vowel_table[x][dau_cau]\n",
    "                # for index2 in vowel_index:\n",
    "                #     if index2 != index:\n",
    "                #         x, y = vowel_to_ids[chars[index]]\n",
    "                #         chars[index2] = vowel_table[x][0]\n",
    "                return ''.join(chars)\n",
    "    \n",
    "        if len(vowel_index) == 2:\n",
    "            if vowel_index[-1] == len(chars) - 1:\n",
    "                x, y = self.vowel_to_ids[chars[vowel_index[0]]]\n",
    "                chars[vowel_index[0]] = self.vowel_table[x][dau_cau]\n",
    "                # x, y = vowel_to_ids[chars[vowel_index[1]]]\n",
    "                # chars[vowel_index[1]] = vowel_table[x][0]\n",
    "            else:\n",
    "                # x, y = vowel_to_ids[chars[vowel_index[0]]]\n",
    "                # chars[vowel_index[0]] = vowel_table[x][0]\n",
    "                x, y = self.vowel_to_ids[chars[vowel_index[1]]]\n",
    "                chars[vowel_index[1]] = self.vowel_table[x][dau_cau]\n",
    "        else:\n",
    "            # x, y = vowel_to_ids[chars[vowel_index[0]]]\n",
    "            # chars[vowel_index[0]] = vowel_table[x][0]\n",
    "            x, y = self.vowel_to_ids[chars[vowel_index[1]]]\n",
    "            chars[vowel_index[1]] = self.vowel_table[x][dau_cau]\n",
    "            # x, y = vowel_to_ids[chars[vowel_index[2]]]\n",
    "            # chars[vowel_index[2]] = vowel_table[x][0]\n",
    "        return ''.join(chars)\n",
    "    \n",
    "    \n",
    "    def is_valid_vietnam_word(self , word):\n",
    "        chars = list(word)\n",
    "        vowel_index = -1\n",
    "        for index, char in enumerate(chars):\n",
    "            x, y = self.vowel_to_ids.get(char, (-1, -1))\n",
    "            if x != -1:\n",
    "                if vowel_index == -1:\n",
    "                    vowel_index = index\n",
    "                else:\n",
    "                    if index - vowel_index != 1:\n",
    "                        return False\n",
    "                    vowel_index = index\n",
    "        return True\n",
    "    \n",
    "    def normalize_sign_of_a_sen(self , sentence):\n",
    "        \"\"\"\n",
    "            Chuyển câu tiếng việt về chuẩn gõ dấu kiểu cũ.\n",
    "            :param sentence:\n",
    "            :return:\n",
    "            \"\"\"\n",
    "        sentence = sentence.lower()\n",
    "        words = sentence.split()\n",
    "        for index, word in enumerate(words):\n",
    "            cw = re.sub(r'(^\\{P}*)([{L}.]*\\{L}+)(\\{P}*$)', r'\\1/\\2/\\3', word).split('/')\n",
    "            if len(cw) == 3:\n",
    "                cw[1] = self.normalize_sign_of_a_word(cw[1])\n",
    "            words[index] = ''.join(cw)\n",
    "        return ' '.join(words)\n",
    "    def remove_special_char(self, document):\n",
    "        special_characters=['@','#','$','*','&']\n",
    "        normal_string=document\n",
    "        for i in special_characters:\n",
    "    # Replace the special character with an empty string\n",
    "            normal_string=normal_string.replace(i,\"\")\n",
    "        return normal_string\n",
    "    def remove_emoji(self, document):\n",
    "        return self.emoji_pattern.sub(r'', document)\n",
    "    def remove_url(self, document):\n",
    "        url_pattern = re.compile(r'http\\S+')\n",
    "        return url_pattern.sub(r'', document)\n",
    "\n",
    "    def remove_stopwords(self , doc):\n",
    "        self.stopwords = [i.replace(\"\\n\",\"\") for i in self.stopwords]\n",
    "        return [w.lower() for w in doc if w not in self.stopwords]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = BaseVNPreprocessor()\n",
    "def text_preprocess(document):\n",
    "    \"\"\"\n",
    "        Tiền xử lý document\n",
    "        --------\n",
    "        input: nhận vào một document là một chuỗi ký tự\n",
    "        output: một mảng các tokens có được từ tiền xử lý document\n",
    "\n",
    "    \"\"\"\n",
    "    # # normalize unicode\n",
    "    document = preprocessor.convert_unicode(document)\n",
    "    \n",
    "    # # normalize sign \n",
    "    document = preprocessor.normalize_sign_of_a_sen(document)\n",
    "\n",
    "    # remove special characters\n",
    "    document = preprocessor.remove_special_char(document)\n",
    "\n",
    "    # remove emoji characters and url\n",
    "    document = preprocessor.remove_emoji(document)\n",
    "    document = preprocessor.remove_url(document)\n",
    "    # split words\n",
    "    document = ViTokenizer.tokenize(document)\n",
    "\n",
    "    # remove unnecessary words\n",
    "    document = re.sub(r'[^\\s\\wáàảãạăắằẳẵặâấầẩẫậéèẻẽẹêếềểễệóòỏõọôốồổỗộơớờởỡợíìỉĩịúùủũụưứừửữựýỳỷỹỵđ_]',' ',document)\n",
    "\n",
    "    # # remove extra space\n",
    "    document = re.sub(r'\\s+', ' ', document).strip()\n",
    "    \n",
    "    # tokenize \n",
    "    document = word_tokenize(document)\n",
    "    \n",
    "    # remove stopwords\n",
    "    document = preprocessor.remove_stopwords(document)\n",
    "    # sửa lỗi chỉnh tả tiếng việt\n",
    "    \n",
    "\n",
    "    return document"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kết nối với Azure SQL Server (Intrieval Information)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyodbc #driver Microsoft cung cấp giúp kết nối giữa csdl với file code\n",
    "username = '20521183@ms.uit.edu.vn'\n",
    "connection = pyodbc.connect(\"Driver={ODBC Driver 17 for SQL Server};Server=tcp:informationretrieval.database.windows.net,1433;Database=News;Uid={\"+\n",
    "username+\"};Encrypt=yes;TrustServerCertificate=no;Connection Timeout=30;Authentication=ActiveDirectoryInteractive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor = connection.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_newsvn ={}\n",
    "for row in cursor.execute(\"select id , title , description from NewsVN\"):\n",
    "       dict_newsvn[row.id]= [row.title ,row.description]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "for id ,item in dict_newsvn.items():\n",
    "    if item[1] == None:\n",
    "        item[1] = ''\n",
    "    dict_newsvn[id] = item[0] + \" \" + item[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "for id , item in dict_newsvn.items():\n",
    "    dict_newsvn[id] = text_preprocess(item)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
